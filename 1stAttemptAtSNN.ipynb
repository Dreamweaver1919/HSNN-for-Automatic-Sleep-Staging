{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# For reproducibility in cuDNN (may slow down performance)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------    Model Definition  -------------------------------\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import mne\n",
    "\n",
    "from spikingjelly.activation_based import encoding,neuron,surrogate,layer\n",
    "from scipy.io import loadmat\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "# Import spikingjelly modules\n",
    "from spikingjelly.activation_based import neuron, surrogate\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "#  2. Reservoir SNN Module using spikingjelly\n",
    "#   ----------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from spikingjelly.clock_driven import neuron, surrogate\n",
    "\n",
    "class SpikingJellyReservoirSNN(nn.Module):\n",
    "    def __init__(self, n_channels, n_reservoir, sfreq, tau=0.02, threshold=1.0):\n",
    "        \"\"\"\n",
    "        n_channels: number of EEG channels\n",
    "        n_reservoir: number of reservoir neurons\n",
    "        sfreq: sampling frequency in Hz\n",
    "        tau: membrane time constant (seconds)\n",
    "        threshold: spiking threshold\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_reservoir = n_reservoir\n",
    "        self.sfreq = sfreq\n",
    "        self.dt = 1.0 / sfreq\n",
    "        self.tau = tau\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Fully-connected input and recurrent layers\n",
    "        self.fc_in = nn.Linear(n_channels, n_reservoir)\n",
    "        self.fc_rec = nn.Linear(n_reservoir, n_reservoir)\n",
    "\n",
    "        # Freeze input and recurrent weights (reservoir remains fixed)\n",
    "        for param in self.fc_in.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.fc_rec.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Optional: Initialize weights for better reservoir dynamics\n",
    "        with torch.no_grad():\n",
    "            # Xavier initialization for input weights\n",
    "            nn.init.xavier_uniform_(self.fc_in.weight)\n",
    "            # Orthogonal initialization for recurrent weights\n",
    "            nn.init.orthogonal_(self.fc_rec.weight)\n",
    "            # Optionally scale to control spectral radius (e.g., 0.9)\n",
    "            with torch.no_grad():\n",
    "                weight = self.fc_rec.weight.data\n",
    "                abs_eigenvalues = torch.abs(torch.linalg.eigvals(weight))\n",
    "                spectral_radius = abs_eigenvalues.max()\n",
    "                self.fc_rec.weight.data = (weight / spectral_radius) * 0.9\n",
    "\n",
    "        # Convert tau to timesteps\n",
    "        tau_timesteps = tau * sfreq\n",
    "        # Create spiking neuron using spikingjelly's LIFNode\n",
    "        self.lif = neuron.LIFNode(tau=tau_timesteps, surrogate_function=surrogate.ATan(), detach_reset=True)\n",
    "        self.lif.v_threshold = threshold  # Set the threshold manually\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, n_channels, time_steps)\n",
    "        Returns: spike counts per neuron (batch_size, n_reservoir)\n",
    "        \"\"\"\n",
    "        batch_size, _, time_steps = x.shape\n",
    "        device = x.device\n",
    "        spike_counts = torch.zeros(batch_size, self.n_reservoir, device=device)\n",
    "        spikes = torch.zeros(batch_size, self.n_reservoir, device=device)\n",
    "\n",
    "        # Reset LIF state at the start of each forward pass\n",
    "        self.lif.reset()\n",
    "\n",
    "        for t in range(time_steps):\n",
    "            input_t = x[:, :, t]  # shape: (batch_size, n_channels)\n",
    "            # Compute input current without ReLU\n",
    "            I_in = self.fc_in(input_t) + self.fc_rec(spikes)\n",
    "            # Update LIF neuron state\n",
    "            spikes = self.lif(I_in)\n",
    "            # Accumulate spike counts\n",
    "            spike_counts += spikes\n",
    "\n",
    "        return spike_counts\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "class ComplexClassifier(nn.Module):\n",
    "    def __init__(self, n_reservoir, n_classes):\n",
    "        super().__init__()\n",
    "        # First hidden layer with 128 neurons\n",
    "        self.fc1 = nn.Linear(n_reservoir, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        # Second hidden layer with 64 neurons\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        #Third hidden layer\n",
    "        self.fc3 = nn.Linear(32, n_classes)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through first hidden layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        # Pass through second hidden layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        # Final output layer; no activation if using a loss that applies one (e.g., CrossEntropyLoss)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Combined Simplified SNN Model using spikingjelly reservoir\n",
    "# ----------------------------\n",
    "class SimplifiedSNNModel(nn.Module):\n",
    "    def __init__(self, n_channels, n_reservoir, n_classes, sfreq, tau=0.02, threshold=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoding.PoissonEncoder()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(n_channels, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.reservoir = SpikingJellyReservoirSNN(64, n_reservoir, sfreq, tau, threshold)\n",
    "        self.classifier = ComplexClassifier(n_reservoir, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, n_channels, time_steps)\n",
    "\n",
    "        x = self.cnn(x)\n",
    "        #x = self.encoder(x)  # Encode the input using Poisson encoding\n",
    "\n",
    "        spike_counts = self.reservoir(x)\n",
    "        # Use the classifier to map the reservoir's spike counts to rate-coded outputs.\n",
    "        rates = self.classifier(spike_counts)\n",
    "        return rates\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Custom Dataset for EEG Trials\n",
    "# ----------------------------\n",
    "class EEGDataset:\n",
    "    def __init__(self, trials):\n",
    "        \"\"\"\n",
    "        trials: list of tuples (segment, label)\n",
    "          segment: numpy array of shape (n_channels, time_steps)\n",
    "          label: integer label for sleep stage\n",
    "        \"\"\"\n",
    "        self.trials = trials\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trials)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        segment, label = self.trials[idx]\n",
    "        segment_tensor = torch.tensor(segment, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return segment_tensor, label_tensor\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/fwx/work/01_sleep_psg.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 5557599  =      0.000 ... 27787.995 secs...\n",
      "Number of epochs (trials): 926\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------  Dataset class Definition & Dataset loading ------------------------\n",
    "# 6. Data Loading: EEG (.edf) and Sleep Staging (.mat)\n",
    "# ----------------------------\n",
    "# Update the file paths as needed.\n",
    "edf_file = os.path.join(os.getcwd(), r\"01_sleep_psg.edf\")\n",
    "try:\n",
    "    raw = mne.io.read_raw_edf(edf_file, preload=True)\n",
    "except Exception as e:\n",
    "    print(\"Error reading EDF file:\", e)\n",
    "    raise\n",
    "\n",
    "eeg_data = raw.get_data()  # shape: (n_channels, total_samples)\n",
    "sfreq = raw.info['sfreq']\n",
    "\n",
    "# Load sleep staging information from a MAT file\n",
    "mat_file = os.path.join(os.getcwd(), r\"01_SleepStages.mat\")\n",
    "mat_contents = loadmat(mat_file)\n",
    "keys = [key for key in mat_contents.keys() if not key.startswith('__')]\n",
    "if len(keys) == 0:\n",
    "    raise ValueError(\"No valid variables found in the MAT file.\")\n",
    "stages_raw = mat_contents[keys[0]]\n",
    "stages_raw = np.squeeze(stages_raw)\n",
    "stages = [str(s) for s in stages_raw]\n",
    "stages_clean = [s[0] if isinstance(s, (np.ndarray, list)) else s for s in stages]\n",
    "stage_mapping = {\"['NREM 1']\": 0, \"['NREM 2']\": 1, \"['NREM 3']\": 2, \"['Wake']\": 3, \"['REM']\": 4}\n",
    "stages_int = [stage_mapping[s] for s in stages_clean]\n",
    "\n",
    "# Build trials: divide continuous EEG data into 30-second epochs\n",
    "epoch_duration = 30  # seconds\n",
    "samples_per_epoch = int(sfreq * epoch_duration)\n",
    "total_samples = eeg_data.shape[1]\n",
    "n_epochs_available = total_samples // samples_per_epoch\n",
    "n_epochs = min(len(stages_int), n_epochs_available)\n",
    "print(f\"Number of epochs (trials): {n_epochs}\")\n",
    "\n",
    "trials = []\n",
    "for epoch_idx in range(n_epochs):\n",
    "    start_idx = epoch_idx * samples_per_epoch\n",
    "    end_idx = (epoch_idx + 1) * samples_per_epoch\n",
    "    segment = eeg_data[:, start_idx:end_idx]\n",
    "    stage = stages_int[epoch_idx]\n",
    "    trials.append((segment, stage))\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = EEGDataset(trials)\n",
    "\n",
    "# ----------------------------\n",
    "# 6.1 Compute Global Normalization Statistics (per channel)\n",
    "# ----------------------------\n",
    "# Stack all segments to compute global mean and std per channel.\n",
    "all_segments = [segment for segment, _ in dataset]\n",
    "all_segments = np.stack(all_segments, axis=0)  # shape: (n_trials, n_channels, time_steps)\n",
    "global_mean = np.mean(all_segments, axis=(0, 2))  # shape: (n_channels,)\n",
    "global_std = np.std(all_segments, axis=(0, 2))    # shape: (n_channels,)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Split Dataset into Training and Evaluation Sets\n",
    "# ----------------------------\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "split_ratio = 0.8  # 80% training, 20% evaluation\n",
    "train_split = int(len(indices) * split_ratio)\n",
    "train_indices = indices[:train_split]\n",
    "eval_indices = indices[train_split:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Initialization\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_channels = eeg_data.shape[0]\n",
    "n_reservoir = 512  # e.g., 512 reservoir neurons\n",
    "n_classes = len(np.unique(stages_int))\n",
    "\n",
    "model = SimplifiedSNNModel(n_channels, n_reservoir, n_classes, sfreq, tau=0.02, threshold=1.0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:   0%|          | 0/23 [00:00<?, ?it/s]/tmp/ipykernel_1194688/3821174872.py:28: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  segment = (segment - global_mean[:, None]) / global_std[:, None]\n",
      "Training Batches: 100%|██████████| 23/23 [01:16<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3948, Training Accuracy: 36.49%\n",
      "\n",
      "Training for epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 23/23 [00:45<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.2889, Training Accuracy: 36.08%\n",
      "\n",
      "Training for epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 23/23 [01:16<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.2344, Training Accuracy: 35.41%\n",
      "\n",
      "Training for epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 23/23 [00:54<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.2049, Training Accuracy: 36.76%\n",
      "\n",
      "Training for epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 23/23 [00:59<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.1971, Training Accuracy: 39.59%\n",
      "\n",
      "Training for epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 23/23 [01:21<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.1806, Training Accuracy: 38.11%\n",
      "\n",
      "Training for epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches: 100%|██████████| 23/23 [00:26<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.1723, Training Accuracy: 39.73%\n",
      "\n",
      "Training for epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:  83%|████████▎ | 19/23 [00:48<00:11,  2.85s/it]"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 8. Training the Simplified Model with Rate Coding Loss (Manual Batching)\n",
    "# ----------------------------\n",
    "\n",
    "criterion = nn.MSELoss()  # Rate coding loss (MSELoss)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs_train = 50  # number of training epochs\n",
    "batch_size = 32\n",
    "\n",
    "model.train()\n",
    "early_stop = False\n",
    "for epoch in range(n_epochs_train):\n",
    "    print(f\"\\nTraining for epoch: {epoch+1}\")\n",
    "    total_loss = 0.0\n",
    "    training_correct = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    num_batches = len(train_indices) // batch_size\n",
    "\n",
    "    for batch_idx in tqdm.tqdm(range(num_batches), desc=\"Training Batches\"):\n",
    "        batch_indices = train_indices[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "        batch_segments = []\n",
    "        batch_labels = []\n",
    "        for idx in batch_indices:\n",
    "            segment, label = dataset[idx]\n",
    "            # Normalize each segment per channel\n",
    "            segment = (segment - global_mean[:, None]) / global_std[:, None]\n",
    "            batch_segments.append(segment)\n",
    "            batch_labels.append(label)\n",
    "        batch_segments = torch.stack(batch_segments)  # shape: [batch_size, n_channels, time_steps]\n",
    "        batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "\n",
    "        batch_segments = batch_segments.to(device, non_blocking=True)\n",
    "        batch_labels = batch_labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_segments)\n",
    "        # Create one-hot targets for rate coding\n",
    "        target = torch.zeros_like(outputs)\n",
    "        target.scatter_(1, batch_labels.view(-1, 1), 1.0)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_segments.size(0)\n",
    "\n",
    "        # Compute predictions and accumulate correct predictions\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        training_correct += (preds == batch_labels).sum().item()\n",
    "        total_train_samples += batch_segments.size(0)\n",
    "\n",
    "    epoch_loss = total_loss / len(train_indices)\n",
    "    training_accuracy = training_correct / len(train_indices) * 100\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Training Accuracy: {training_accuracy:.2f}%\")\n",
    "    \n",
    "    # Early stopping if training accuracy is above 85%\n",
    "    if training_accuracy >= 85:\n",
    "        print(\"Training accuracy above 85%. Early stopping!\")\n",
    "        # Print out the parameter list (state dictionary)\n",
    "        print(\"Model parameters:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            print(f\"{name}: {param.data}\")\n",
    "        early_stop = True\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Use CrossEntropyLoss instead of MSELoss\n",
    "# ----------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "n_epochs_train = 50  # number of training epochs\n",
    "batch_size = 32\n",
    "# ----------------------------\n",
    "# 8. Training the Simplified Model with Cross-Entropy Loss (Manual Batching)\n",
    "# ----------------------------\n",
    "model.train()\n",
    "early_stop = False\n",
    "for epoch in range(n_epochs_train):\n",
    "    print(f\"\\nTraining for epoch: {epoch+1}\")\n",
    "    total_loss = 0.0\n",
    "    training_correct = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    num_batches = len(train_indices) // batch_size\n",
    "\n",
    "    for batch_idx in tqdm.tqdm(range(num_batches), desc=\"Training Batches\"):\n",
    "        batch_indices = train_indices[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "        batch_segments = []\n",
    "        batch_labels = []\n",
    "        for idx in batch_indices:\n",
    "            segment, label = dataset[idx]\n",
    "            # Normalize using the global mean and std (per channel)\n",
    "            segment = (segment - global_mean[:, None]) / global_std[:, None]\n",
    "            batch_segments.append(segment)\n",
    "            batch_labels.append(label)\n",
    "        batch_segments = torch.stack(batch_segments)  # shape: [batch_size, n_channels, time_steps]\n",
    "        batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "\n",
    "        batch_segments = batch_segments.to(device, non_blocking=True)\n",
    "        batch_labels = batch_labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_segments)  # outputs: raw logits of shape [batch_size, n_classes]\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_segments.size(0)\n",
    "\n",
    "        # Compute predictions and accumulate correct predictions\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        training_correct += (preds == batch_labels).sum().item()\n",
    "        total_train_samples += batch_segments.size(0)\n",
    "\n",
    "    epoch_loss = total_loss / len(train_indices)\n",
    "    training_accuracy = training_correct / len(train_indices) * 100\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Training Accuracy: {training_accuracy:.2f}%\")\n",
    "\n",
    "    # Early stopping condition (optional): stop if training accuracy >= 85%\n",
    "    if training_accuracy >= 85:\n",
    "        print(\"Training accuracy above 85%. Early stopping!\")\n",
    "        print(\"Model parameters:\")\n",
    "        for name, param in model.named_parameters():\n",
    "            print(f\"{name}: {param.data}\")\n",
    "        early_stop = True\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If early stopping was triggered, evaluate on the test set immediately\n",
    "if early_stop:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        num_batches_eval = len(eval_indices) // batch_size\n",
    "        for batch_idx in tqdm.tqdm(range(num_batches_eval), desc=\"Evaluation Batches\"):\n",
    "            batch_indices = eval_indices[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "            batch_segments = []\n",
    "            batch_labels = []\n",
    "            for idx in batch_indices:\n",
    "                segment, label = dataset[idx]\n",
    "                # Normalize using the same global statistics\n",
    "                segment = (segment - global_mean[:, None]) / global_std[:, None]\n",
    "                batch_segments.append(segment)\n",
    "                batch_labels.append(label)\n",
    "            batch_segments = torch.stack(batch_segments)\n",
    "            batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "            batch_segments = batch_segments.to(device, non_blocking=True)\n",
    "            batch_labels = batch_labels.to(device, non_blocking=True)\n",
    "            outputs = model(batch_segments)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == batch_labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct / (num_batches_eval * batch_size) * 100\n",
    "    print(f\"\\nEvaluation Accuracy (Test Set): {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Evaluation without stopping -------\n",
    "# 9. Evaluate the Simplified Model with Rate Coding (Manual Batching)\n",
    "# ----------------------------\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    num_batches_eval = len(eval_indices) // batch_size\n",
    "    for batch_idx in tqdm.tqdm(range(num_batches_eval), desc=\"Evaluation Batches\"):\n",
    "        batch_indices = eval_indices[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "        batch_segments = []\n",
    "        batch_labels = []\n",
    "        for idx in batch_indices:\n",
    "            segment, label = dataset[idx]\n",
    "            # Use the same global normalization\n",
    "            segment = (segment - global_mean[:, None]) / global_std[:, None]\n",
    "            batch_segments.append(segment)\n",
    "            batch_labels.append(label)\n",
    "        batch_segments = torch.stack(batch_segments)\n",
    "        batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "        batch_segments = batch_segments.to(device, non_blocking=True)\n",
    "        batch_labels = batch_labels.to(device, non_blocking=True)\n",
    "        outputs = model(batch_segments)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == batch_labels).sum().item()\n",
    "\n",
    "accuracy = correct / (num_batches_eval * batch_size) * 100\n",
    "print(f\"\\nEvaluation Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple other classifiers\n",
    "# We'll define a helper function to extract features from the reservoir.\n",
    "def extract_reservoir_features(model, indices, batch_size=32):\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(indices) // batch_size\n",
    "        for batch_idx in tqdm.tqdm(range(num_batches), desc=\"Extracting Features\"):\n",
    "            batch_indices = indices[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "            batch_segments = []\n",
    "            batch_labels = []\n",
    "            for idx in batch_indices:\n",
    "                segment, label = dataset[idx]\n",
    "                segment = (segment - global_mean[:, None]) / global_std[:, None]\n",
    "                batch_segments.append(segment)\n",
    "                batch_labels.append(label)\n",
    "            batch_segments = torch.stack(batch_segments)  # shape: [batch_size, n_channels, time_steps]\n",
    "            batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "            batch_segments = batch_segments.to(device, non_blocking=True)\n",
    "            # Extract reservoir features directly from the reservoir module.\n",
    "            features = model.reservoir(batch_segments)\n",
    "            features_list.append(features.cpu().numpy())\n",
    "            labels_list.append(batch_labels.cpu().numpy())\n",
    "    X = np.concatenate(features_list, axis=0)\n",
    "    y = np.concatenate(labels_list, axis=0)\n",
    "    return X, y\n",
    "\n",
    "# Extract features from training and evaluation sets\n",
    "X_train, y_train = extract_reservoir_features(model, train_indices, batch_size)\n",
    "X_eval, y_eval = extract_reservoir_features(model, eval_indices, batch_size)\n",
    "\n",
    "# Train classical classifiers using scikit-learn.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- 1. Logistic Regression ---\n",
    "clf_lr = LogisticRegression(max_iter=1000)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "y_pred_lr = clf_lr.predict(X_eval)\n",
    "accuracy_lr = accuracy_score(y_eval, y_pred_lr)\n",
    "print(f\"\\nLogistic Regression Accuracy: {accuracy_lr:.2f}\")\n",
    "\n",
    "# --- 2. Random Forest ---\n",
    "clf_rf = RandomForestClassifier(n_estimators=100)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_pred_rf = clf_rf.predict(X_eval)\n",
    "accuracy_rf = accuracy_score(y_eval, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.2f}\")\n",
    "\n",
    "# --- 3. Support Vector Machine ---\n",
    "clf_svm = SVC(kernel='linear')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "y_pred_svm = clf_svm.predict(X_eval)\n",
    "accuracy_svm = accuracy_score(y_eval, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
